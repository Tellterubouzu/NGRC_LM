ざっくり順番に答えると：
	•	コンテキスト長 ≒ 遅延長 L ではない（入力系列分 L を取るのはほぼ現実的じゃない）
	•	多項式次数 P はほぼ 1 or 2 に絞るのが現実的（P=3 以上はまず死ぬ）
	•	多項式特徴の一部だけを選ぶ仕組みはアリだけど、
「学習後に静的に間引く」形にしないと GPU 的にはあまり速くならない

という感じです。順に細かく書きます。

⸻

1. 「ほぼ無限コンテキスト」と遅延長 L の関係

1.1 L = コンテキスト長にはしない方がいい

もし

「コンテキスト長 T トークンにしたいから L = T の遅延長を取る」

とすると、1 レイヤの入力次元 D に対して
	•	生の遅延ベクトル：D × L
	•	2 次多項式まで取ると（フルにやるなら）O((D L)²)

になって、一瞬で破綻します。

Long-context 性能は「L だけ」で決まるわけではなく、
“状態ベクトル h_t にどれだけ過去情報を圧縮して持たせられるか” で決まるので、
	•	L は「ごく近い過去をどの粒度で見るか」を決めるパラメータ
	•	それより前の情報は h_t 自体がキャリーする（RNN と同じ発想）

と考える方が現実的です。

1.2 L をそんなに伸ばさずに「ほぼ無限」に近づくには？

ざっくり言うと：
	1.	隠れ状態 h_t をちゃんとリカレントにする
	•	多項式特徴は
\phi_t = \text{poly}([h_t, h_{t-1}, ..., h_{t-L+1}])
みたいにして、
h_t が「過去全部の圧縮サマリ」になるように学習させる
	•	L は 4〜16 程度でも、h_t の設計次第で数千トークンの情報を持てる
	2.	ディレイをマルチスケールにする
	•	1,2,4,8,16,… みたいな「指数的ラグ」だけ使う
（Dilated conv / TCN と同じ発想）
	•	例えば {1,2,4,8,16,32,64,128} の 8 個だけをラグにすれば、
L=8 でも “128 トークン先まで” の情報を直接覗ける。
	•	それに加えて h_t がもっと長期の記憶を持つ。
	3.	状態の減衰付きリカレンス
	•	h_{t+1} = α h_t + f(現在入力, 多項式特徴) のように
α<1 を入れて「無限長だけど指数減衰する記憶」にする
（SSM / S4 / RWKV 系と近い発想）

なので、

long-context を狙うからといって L を入力系列長分取るのは現実的ではないし、
そうしなくても「h_t の設計＋マルチスケール遅延」で十分長い依存を持つことは可能

という答えになります。

⸻

2. 多項式次数 P はどう選ぶ？

2.1 P=1,2,3… のざっくりイメージ
	•	P=1
	•	ただの linear AR（＋リカレント線形層）
	•	非線形性が足りないので、NLG 的には厳しいと思っておいた方がいい
	•	P=2
	•	“二次までの Volterra 系” に相当して、かなりリッチ
	•	カオス系のモデリングでも P=2,3 くらいまでで十分なことが多い
	•	計算量と表現力のバランス的に 現実的な上限はほぼ P=2
	•	P≥3
	•	特徴次元が爆発的に増える
	•	120M パラで NLG を回すスケールだと、よほど工夫しないと無理

現実的には：

ベースラインとしては P=2 固定で、
その代わり「どの二次項を取るか」をめちゃくちゃ絞る

という方が設計しやすいです。

2.2 120M デモを前提にしたざっくり目安

例えば hidden 次元 H=768 前後を想定するとして：
	•	時間方向のラグ数 L=4〜8
	•	各チャネルごとに
[h_t^{(j)}, h_{t-1}^{(j)}, ..., h_{t-L+1}^{(j)}] をとり
その中だけで 2 次多項式 \Rightarrow 1 チャンネルあたり特徴数
	•	L=4 → 4 + 4·5/2 = 14
	•	L=8 → 8 + 8·9/2 = 44
	•	全チャネルで
M ≈ H × 14（〜1万） or H × 44（〜3.4万）

レイヤ構成として
	•	\phi_t \in \mathbb{R}^M
	•	線形で H に戻す：W: H×M

のパラメータ数は
	•	L=4 → 約 768×1.1万 ≈ 8.5M / layer
	•	L=8 → 約 768×3.4万 ≈ 26M / layer

12 layer だと L=4 でも 100M 近く飛ぶので、

	•	P=2
	•	L は 4 前後
	•	レイヤ数 6〜8
	•	各レイヤの中で一度ボトルネックに落とす

みたいな設計にしないと 120M には収まりません。

⸻

3. 多項式特徴の一部だけを選ぶことで計算量は減らせるか？

結論：やり方次第でかなり減らせるが、「動的な選択」は GPU 的には微妙。
静的に間引いて、dense な小さい行列に再コンパイルするのが現実的。

3.1 事前に「構造的に絞る」パターン

これは一番簡単で速い方法です。
	•	時間方向のみの多項式
	•	各チャネル independent に「同一チャネルのラグ同士の積」だけ取る
	•	チャネル間クロス（h_t^i × h_{t-1}^j, i≠j）は全部捨てる
→ さっき書いたような「44 などの小さな固定数 / チャネル」設計
	•	ラグの種類を絞る
	•	最近の数ステップだけはフルな 2 次項
	•	離れたラグは 1 次だけ or ごく一部の 2 次だけ
など、マルチスケールで構造的に sparse にする

こういう “最初から計算しない項” を設計で決めてしまうやり方は、
	•	実装はただの element-wise 演算＋固定の dense matmul
	•	ブランチがないので GPU で速く、実装も簡単

というメリットがあります。

3.2 学習で「どの特徴を生かすか」を決める

もう一段攻めるなら、例えば：
	•	多項式特徴ベクトル \phi \in \mathbb{R}^M
	•	各特徴ごとにゲート g_k（0〜1）を持たせて
\tilde{\phi}_k = g_k \cdot \phi_k
	•	g_k に L1（あるいは group lasso 的な）正則化をかけて
多くの g_k を 0 に押し込む

みたいな仕組みで、どの特徴が効いているかを学習することはできます。

ただしこの方式だと：
	•	学習中：M 次元全部計算した上で g_k をかけるので、計算量は減らない
	•	推論時：g_k がほぼ 0 な次元は飛ばせるが、
分岐を多用すると warp divergence で遅くなりがち

→ 実際に計算量を減らしたいなら、
	1.	学習完了後に「小さい g_k をまとめて pruning」
	2.	その結果だけを使って「縮小された特徴集合用に行列／カーネルを再構築」
	3.	推論時は最初から「小さくなった M’ 次元」前提のコードで動かす

という 2 ステージ方式（learn → prune → recompile） がおすすめです。

3.3 低ランク・ランダム特徴で間接的に減らす

フルな 2 次多項式ではなく、
	•	ランダム or 学習済み射影 R \in \mathbb{R}^{d_r \times (D L)} で x をまず d_r 次元に落とす
	•	そのあと (Rx)⊙(Rx) のような「低ランクな 2 次項」だけを取る

という形にすると、
	•	フルな 2 次多項式に比べて特徴数を d_r に抑えられる
	•	うまくやれば「ポリノミアルカーネルの低ランク近似」になっている

ので、**“必要な多項式成分だけを暗黙的に選んでいる”**のと同じ効果が得られます。
これはカーネル法でよくやる random feature の発想です。

⸻

4. 120M デモ用にざっくり設計するなら（1 つの例）

かなりラフですが、例えば：
	•	語彙：32k
	•	埋め込み・隠れ次元：H = 768
	•	Embedding と出力を tying：32k×768 ≈ 25M パラメータ
	•	残り ~95M をモデル本体に使えるとする

各 NGRC レイヤを：
	1.	入力：h_t ∈ R^768
	2.	ラグ：L=4（[h_t, h_{t-1}, h_{t-2}, h_{t-3}]）
	3.	各チャネル independent に 2 次までの多項式 → 14 特徴 / チャネル
→ φ_t ∈ R^{768×14} ≈ R^{10,752}
	4.	線形で 10,752 → 768 に圧縮（+ GeLU + LayerNorm）
→ W: 10,752×768 ≈ 8.3M パラ / layer

これを 8 層積むと
	•	モデル本体：8.3M×8 ≈ 66M
	•		•	各層の LayerNorm / bias など：数 M
	•		•	Embedding / LM head：25M

合計 100M ちょい〜110M くらいで収まりそうです（かなりざっくりですが）。

ここから
	•	どこかの層で L=8 のレイヤを混ぜて「中距離専用」レイヤを作る
	•	上位 2 層だけ多項式を少しリッチにして長距離依存を補う

みたいにアレンジしていくイメージです。

⸻

まとめ
	•	遅延長 L を入力系列長と同じにするのはほぼ非現実的
→ L は「近傍＋マルチスケール」の範囲に抑え、
長期記憶は h_t 自体のリカレンスで持たせる
	•	多項式次数 P は 2 までが現実的な上限
→ P=2 固定にして、どの項を取るかを絞る方がいい
	•	多項式特徴の一部だけを選んで計算量を減らすことは可能
	•	事前に構造設計で絞る
	•	あるいは L1 / group-lasso で “どの特徴が効いているか” を学習 →
後から pruning & 再コンパイルして本当に計算を減らす
	•	低ランクなポリノミアル近似（random features）もアリ

この辺を前提に、
「120M で収まる具体的な設定を何パターンか出して FLOPs/トークンを見積もる」
ところまでやると、Transformer との現実的な比較もかなり見えやすくなると思います。