結論から言うと、

「それなりの表現力を持つ多層 NGRC で GPT‑2 クラスを狙う」前提だと、たいてい GPT‑2 型 Transformer の方が速い
（＝FLOPs もパラメータも少なく、GPU 向き）

です。
NGRC 側をかなり“ケチった”設計にすれば GPT‑2 より軽くすることもできますが、そのぶん表現力をどこまで確保できるかが怪しくなります。

⸻

1. ざっくり FLOPs 比較イメージ

GPT‑2 small（例）
	•	層数：12
	•	隠れ次元：768
	•	FFN 次元：3072
	•	1 トークン・1 層あたりの主な計算：
	•	Q/K/V の線形変換
	•	attention 出力の線形変換
	•	FFN（768 → 3072 → 768）

self‑attention の QK^T・softmax もありますが、KV キャッシュありの生成時なら、
FLOPs はざっくり

1 トークンあたり ≈ 数 × 10⁷ FLOPs

のオーダーになります（12 層合計で ~10⁸ FLOPs/トークンくらい）。

⸻

NGRC（多層・2 次多項式・遅延あり）を素直に設計した場合

例えば：
	•	各層の隠れ次元：768
	•	遅延長 L = 8
	•	多項式次数 P = 2
	•	特徴設計：次元ごとに「時間方向のクロス項を全部取る」（NGRC の典型的なやり方に近い）

とすると、1 チャンネルあたりの特徴数は
	•	1 次項：L
	•	2 次項：L(L+1)/2

で合計 44 特徴 / チャンネル。
隠れ次元 768 なら、1 トークンで得られる多項式特徴は

M ≈ 768 × 44 ≈ 3.4 万次元

になります。

これを 768 次元に戻す線形層（W: 768×33792）を通すと、
	•	1 層での matmul FLOPs ≈ 2.6×10⁷ FLOPs

となり、1 層あたりの計算量は GPT‑2 の 3〜4 倍くらいになります。
12 層積むと、ざっくり

NGRC：3×10⁸ FLOPs / トークン
vs
GPT‑2 small：1×10⁸ FLOPs / トークン

というイメージで、そのまま比較すると NGRC の方がかなり重いです。

※ 多項式特徴を計算するコスト（掛け算）は、この規模だと matmul に比べれば誤差レベルです。
　実質、「多項式で次元を 44 倍に膨らませたせいで、線形層が超デカくなる」のが効いています。

⸻

2. トレーニング vs 推論 でのスケーリングの違い

(1) トレーニング時
	•	Transformer (GPT‑2)
	•	self‑attention のせいで O(T² d) の項が効く（T: シーケンス長、d: 隠れ次元）
	•	NGRC（固定遅延 L）
	•	各トークンが見るのは直近 L step だけなので、だいたい O(T L d)（＝O(T)）

「T がすごく長い（数千〜数万トークン）」という状況では、Big‑O 的には NGRC の方がスケール良いです。
ただし上で見たような「多項式で M が 3〜4 万次元に膨れている」設計だと、
	•	T に対しては O(T) だけど
	•	M がデカすぎて 常に M² スケールの matmul が支配的

になり、結局 実時間は GPT‑2 と比べてあまりおいしくない（むしろ遅い）、となりがちです。

(2) 推論時（オートレグレッシブ生成）
	•	GPT‑2
	•	KV キャッシュありでも、各 step で Query × すべての過去 Key の内積を取るため O(T) が残る
	•	とはいえ実装が超最適化されているので、“そこそこ長い T” まではかなり速い
	•	NGRC（遅延 L 固定）
	•	各 step で見るのは長さ L の過去だけ → O(L)
	•	L を 16 や 32 に抑えれば、T によらず一定コスト

なので、「T が極端に長い場合」「L をかなり小さくできる場合」は NGRC に分がありますが、
普通の GPT‑2 用途（〜1k トークン前後）だと、現状の実装品質込みで GPT‑2 の方が速いことが多いと思っておいた方が安全です。

⸻

3. 実装・GPU 的な観点の差

3.1 Transformer の有利な点
	•	全部「線形（GEMM）＋少数の標準演算（softmax, GeLU 等）」なので、
	•	cuBLAS / cuDNN / FlashAttention などで鬼ほど最適化されている
	•	メモリアクセス・カーネル融合もかなり詰められている
	•	つまり **「同じ FLOPs なら Transformer の方が実時間が短くなりやすい」**環境になっている

3.2 NGRC の不利な点
	•	多項式特徴生成は
	•	大きなテンソルに対する要素ごとの掛け算・スライス・concat
	•	自分でカーネルを書いてうまく fused しないとメモリ帯域に殺されがち
	•	多項式で膨れた M を扱う線形層は
	•	パラメータ数も大きく VRAM を食う
	•	GEMM 自体は速いが、勾配計算・通信などトータルのボトルネックになりやすい

そのため、ライブラリ側の最適化をそのまま享受できる GPT‑2 の方が、現実の GPU 上ではかなり有利です。

⸻

4. 「NGRC の方が速くなり得る」条件

逆に、NGRC 側に寄せて設計すれば、状況は少し変わります：
	1.	多項式特徴をかなり制限する
	•	例：
	•	時間方向の 1 次＋ごく一部の 2 次項だけ
	•	クロス項は同じチャネル内で 2〜3 個だけ
	•	こうすると「特徴数 / チャンネル」を 44 → 4〜8 程度まで落とせる
	•	このくらいまで絞ると、1 層あたりの GEMM FLOPs は GPT‑2 と同オーダーか、少し軽いくらいにできる
	2.	各レイヤでボトルネック線形を必ず挟む
	•	多項式で一度広げても、すぐに 512〜1024 次元程度に圧縮する
	•	「広がっているのは内部だけ」で外側の hidden は小さく保つ
	3.	コンテキストはそこまで長くない
	•	L を 16 程度にして、T も 1〜2k 程度
	•	この条件なら attention の O(T) 償却も効くので、
NGRC と GPT‑2 の計算量はだいたい同じオーダー

このくらいの“かなりケチった NGRC”なら、
	•	FLOPs 的には GPT‑2 small と「同程度か、ちょい軽い」くらいに設計できる
	•	ただしその構造で GPT‑2 級の言語モデル性能が出るかは、正直まだ未知数

という感じになります。

⸻

5. まとめ（どっちが速そう？）

整理すると：
	•	素直に NGRC を多層化して GPT‑2 並の表現力を狙うと、次元爆発のせいで GPT‑2 より遅く・重くなりやすい
	•	多項式特徴のせいで 1 層あたりの FLOPs とパラメータが増えすぎる
	•	多項式特徴をかなり制限し、毎層ボトルネックで圧縮するなら、FLOPs だけ見れば GPT‑2 と同程度〜やや軽いところを狙える
	•	ただしその場合、性能とのトレードオフが大きい
	•	さらに、現状の GPU エコシステムは圧倒的に Transformer 寄りに最適化されているので、
	•	「同じ FLOPs・同じパラメータ数」で比べても、実時間は GPT‑2 の方が有利になりがち

なので、

実務的な「速い NLG モデル」が欲しいだけなら、今のところは GPT‑2 系をそのまま使った方が速くて楽。
NGRC で速度メリットを出すには、かなり慎重に特徴設計＆カーネル最適化をしないといけない

というのが現実的な見立てになります。

⸻

もしよければ、
	•	想定している hidden 次元・遅延長・多項式次数
	•	目標コンテキスト長・モデルサイズ（パラメータ数の目安）

を教えてくれれば、その条件で「ざっくり FLOPs / トークン」と「必要 VRAM」を具体的に見積もってみます。