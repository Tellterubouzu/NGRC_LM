いきなり結論っぽく言うと、多層 NGRC で NLG をやろうとするとボトルネック候補はだいたいこのあたりです：
	1.	多項式＋遅延特徴の「次元爆発」とその生成コスト
	2.	リッジ回帰（あるいは線形層）の学習コスト：ΦᵀΦ の構築と解法
	3.	出力層（語彙数 × 特徴次元）の softmax ロジット計算
	4.	オートレグレッシブ生成時のスライディングウィンドウ更新・キャッシュ戦略
	5.	多層化に伴う特徴次元の雪だるま式増加とメモリ帯域

順番に、NGRC を NLG 用の多層モデルにしたときの「どこが重くなるか」をもう少し具体的に書きます。

⸻

0. 想定している NGRC × NLG のざっくり構造

議論の土台として、こんなイメージを置きます：
	•	入力：トークン列 (w_1, \dots, w_T)
	•	埋め込み：e_t \in \mathbb{R}^{D_0}（例：D₀=1024）
	•	各 NGRC レイヤ ℓ で
	1.	遅延埋め込み：[h_t^{(\ell)}, h_{t-1}^{(\ell)}, \dots, h_{t-L+1}^{(\ell)}]
	2.	多項式特徴（NVAR）生成：\phi_t^{(\ell)} \in \mathbb{R}^{M_\ell}
	3.	線形変換＋（場合によって非線形）：
h_t^{(\ell+1)} = W^{(\ell)} \phi_t^{(\ell)} （＋活性化など）
	•	最終層で語彙 V に対して
\text{logits}_t = W_{\text{out}} h_t^{(L)} \in \mathbb{R}^{|V|}

これを前提に、どこが詰まりやすいかを見ていきます。

⸻

1. 多項式＋遅延特徴の爆発と生成・保持コスト

NGRC のコアは
	•	遅延：長さ L のウィンドウ
	•	多項式次数：P（例：2 or 3）

で作る多項式特徴 \phi_t ですが、ここが一番ヤバいボトルネック候補です。

1.1 フルな多項式展開の次元はすぐ死ぬ

元の特徴次元を N = D \times L とすると、2 次までの単純な多項式だけでも
	•	1 次項：N
	•	2 次項：N(N+1)/2

なので、例として D=1024, L=8 → N=8192 とすると
2 次をフルに取ると 33,558,528 特徴になります（1 時刻あたり…）。

これを
	•	1 トークンごとに生成
	•	\Phi として GPU メモリに積む

のはほぼ不可能レベルで、計算量よりもメモリと帯域が完全にボトルネックになります。

1.2 GPU 目線のボトルネック

GPU から見ると：
	•	多項式特徴生成は「巨大テンソルに対する element-wise 乗算・累乗・ペア積」
	•	演算自体は並列化しやすい
	•	しかし読み書きメモリ量が膨大になり、メモリ帯域に支配されやすい
	•	生成した \phi_t を [B, T, M] として保持すると VRAM がすぐ尽きる
	•	レイヤを重ねると M がさらに増えていき、GEMM 前にメモリで死ぬパターン

1.3 現実的に抑え込む方向性

ここを抑えないとそもそも走らないので、ボトルネックであることを前提に設計を変える必要があります：
	•	クロス項を大幅に制限する
	•	dim 間クロスを諦めて「次元ごとの 1D NVAR」にする
（各 embedding 次元について、時間方向だけの多項式：u_t, u_{t-1}, u_t u_{t-1}, \dots）
	•	もしくは「チャネルを小さなグループに分けて、その中だけでクロスを取る」
	•	低ランク・ランダム特徴を挟む
	•	遅延ベクトルを一度ランダム射影 or 学習済み線形で小さい次元 d’ に落としてから多項式展開
→ 次元爆発を d’ 基準に抑える
	•	多項式次数を 2 以下に固定
	•	3 次以上をフルに取るとほぼ終了なので、2 次まで＋スパース選択が現実的

あなたが「GPU に最適化して計算量を抑える」のを主眼にするなら、
NGRC らしさを維持しつつ、ここをいかに“貧弱な”特徴設計にするかが最初の設計勝負になります。

⸻

2. リッジ回帰（readout）の学習：ΦᵀΦ と逆行列

単層 NGRC なら、readout を

W = (\Phi^\top \Phi + \beta I)^{-1} \Phi^\top Y

で一発で解く、というのが “RC っぽさ” ですが、NLP スケールではこれもボトルネックになります。

2.1 計算量スケーリング
	•	Φ: 形状 [T_total, M]
	•	T_total = 全トークン数（億クラスもふつう）
	•	M = 特徴次元
	•	ΦᵀΦ: 形状 [M, M]
	•	計算コスト：O(T_total * M²)
	•	保存コスト：O(M²)

さらに
	•	逆行列 or Cholesky / SVD：O(M³)

なので、M が数万クラスまで膨れると
	•	ΦᵀΦ の計算が「巨大 GEMM」で GPU 的には美味しいが
	•	コストそのものがでかすぎて現実的でない

2.2 メモリ・データセットスケールの問題
	•	Φ 自体を [T_total, M] で持つのは不可能なので、ストリーミングで ΦᵀΦ を集計せざるを得ない
	•	これはまだマシ（バッチごとに ΦᵀΦ の部分和を足し込んでいく）
	•	ただし、巨大なコーパスで何周も回すようなことを考えると
いわゆる “closed-form NGRC” の旨味はだいぶ薄れます

2.3 回避策
	•	「NGRC 風のアーキテクチャだが、学習は普通に SGD / Adamでやる」
	•	Φ を「多項式特徴付き線形層」とみなして end-to-end で勾配降下
	•	そうすると RC というより “固定構造 MLP” ですが、GPU 的には一番自然
	•	どうしても closed-form をやりたいなら
	•	M をかなり抑える（数千〜1 万程度）
	•	スケッチング（ランダム特徴で M をさらに圧縮）
	•	近似ソルバ（共役勾配法とか）で (\Phi^\top \Phi + \beta I)\mathbf{w} = \Phi^\top \mathbf{y} を解く

いずれにせよ、「ΦᵀΦ の計算＋解法」は多層 NGRC ではレイヤごとに発生しうるので、
M をどう抑えるかがここでも効いてきます。

⸻

3. 出力層（語彙 × 特徴次元）のロジット計算

自然言語“生成”タスクなので、最終的に

\text{logits}_t = W_{\text{out}} h_t^{(L)}, \quad
W_{\text{out}} \in \mathbb{R}^{|V| \times H}

を毎トークン計算します。
	•	|V| = 語彙サイズ（3〜5 万とか）
	•	H = 最終隠れ次元（ここが NGRC の M_L）

Transformer でもここがかなり重いですが、
NGRC の場合、多項式特徴のせいで H がデカくなりがちなので、ここが一気にボトルネックになります。
	•	例：H=45056, |V|=50,000 → ざっくり 2.2e9 乗算/トークン
（普通の LLM の H=4096 だと ~2.0e8 乗算/トークンなので 10 倍オーダー）

3.1 抑え込む方向
	•	多項式特徴 → 線形ボトルネックで圧縮
	•	各レイヤの NGRC 出力は一度小さめ次元（例：4096）に射影してから次へ
	•	最終 H を Transformer と同程度に抑える
	•	出力層を低ランク分解 or 語彙クラスタで分割
	•	W_{\text{out}} \approx U V（低ランクロジット）
	•	Adaptive Softmax / sampled softmax で高頻度語優先

ここは Transformer とほぼ同じ悩みですが、
NGRC 側の都合で H が肥大すると一気に支配的なボトルネックになる、という点が重要です。

⸻

4. オートレグレッシブ生成時のスライディングウィンドウ処理

生成フェーズでは、トークンを 1 個ずつ出すので
	•	各ステップで
	•	過去 L ステップ分の hidden を集めて
	•	多項式特徴 \phi_t を作り
	•	線形を通して次の hidden / ロジットを計算

という流れになります。

4.1 素朴にやると O(T²)

何も工夫しないと、
	•	1 ステップごとに「長さ L の window から多項式特徴を全部再計算」
	•	L をシーケンス長 T と同程度にしていると、生成 T ステップで O(T²) 時間

になります。実際の設計では L を固定にすると思いますが、それでも
	•	各ステップで L×多項式演算（＋レイヤ数分）
	•	生成時はバッチサイズ小さいので、GPU を持て余しやすい

4.2 ボトルネックになりやすい点
	•	キャッシュ戦略がショボいとムダ計算が多い
	•	NGRC は「遅延構造」があるので、本来は
	•	過去の \phi_{t-1} から一部だけ更新して \phi_t を作る
	•	hidden のリングバッファで gather する
	•	といった形で O(1) に近づけられる
	•	レイヤを重ねると
	•	各レイヤで別の Lℓ を持つなら、その分バッファも増える
	•	キャッシュしないと「レイヤごとに再計算」で倍々ゲーム

→ 生成フェーズでのボトルネックは
	•	多項式特徴のインクリメンタル更新がちゃんと書けているか
	•	各レイヤの hidden / 遅延をどうキャッシュしているか

にかなり依存します。

⸻

5. 多層化による特徴次元の増加とメモリ帯域

多層 NGRC にすると、
	1.	レイヤ 1：\phi^{(1)} \in \mathbb{R}^{M_1}
	2.	線形で h^{(2)} に圧縮 or 拡張
	3.	レイヤ 2：遅延＋多項式で \phi^{(2)} \in \mathbb{R}^{M_2}
	4.	…

という感じになりますが、素直にやると
	•	各レイヤで「遅延 × 多項式」で Mℓ が膨れる
	•	それをさらに次レイヤの入力として扱うので、次元が雪だるま式に膨らむ

→ 結果的に
	•	前向き計算の GEMM が巨大になる
	•	それ以上に、[B, T, Mℓ] を全部保持しようとすると VRAM が足りない
	•	GPU 的には「計算よりもメモリ帯域」が支配的

5.1 抑えるためのアーキテクチャ上の工夫
	•	毎レイヤ「多項式→ボトルネック線形」を挟む
	•	例：多項式特徴 M_poly=50k → 線形で H=2048 に圧縮
	•	次のレイヤは H と遅延だけを見て再び多項式
	•	“多項式は下位レイヤだけ”に限定
	•	レイヤ 1〜2 だけ NGRC（多項式＋遅延）
	•	その上は普通の線形ブロック or 軽い MLP/Attention
	•	レイヤごとに遅延長 Lℓ を短くする
	•	上位レイヤは短いコンテキスト＋小さい多項式だけを見る

こうしないと、「多層にするほど NGRC ならではの計算負荷」で潰れます。

⸻

6. GPU に寄せるときの実装上のボトルネックまとめ

ここまでを “GPU 実装” の観点だけで整理すると：
	1.	多項式特徴生成カーネル
	•	できるだけ
	•	broadcasting された element-wise 演算
	•	バッチと sequence 次元をまとめた大きなテンソルに対する一括カーネル
	•	にして、カーネル起動のオーバーヘッドを減らす
	•	分岐の多い「if ベースの特徴選択」は避ける（divergence）
	2.	ΦᵀΦ / ΦᵀY の GEMM
	•	“一発で解く RC らしさ” を維持する場合、ここは巨大 GEMM になるので
	•	バッチサイズ・シーケンスを工夫して GPU を一杯に使えるようにする
	•	ただし M を抑えないとそもそも計算量が爆発
	3.	出力層 GEMM（H × |V|）
	•	これは Transformer と同様に必ず重い
	•	H と |V| の両方を意識して設計しないとここが完全なボトルネックに
	4.	オートレグレッシブ生成時のメモリアクセス
	•	遅延ウィンドウの gather / shift を
	•	1D のリングバッファ
	•	連続したメモリレイアウト
	•	にうまく落とし込むかどうかで、生成速度がかなり変わる

⸻

7. ざっくりした設計指針（計算量を抑えたい前提）

ボトルネックを踏まえた上で、もし自分が「NGRC で NLG」をやるなら、スタンスとしては：
	•	多項式部分を“かなりケチる”ことを前提にする
	•	次数 ≤ 2
	•	時間方向だけクロス or ごく局所的なチャネルグループだけクロス
	•	各レイヤで必ずボトルネック線形を挟む
	•	多項式 → H_small → 次レイヤ
	•	（実務的には）closed-form の解法は捨てて SGD / Adam に寄せる
	•	architecture は NGRC 風だが training は普通の NN と同じ
	•	そうするとボトルネックは「特徴生成カーネル」と「出力層 GEMM」の 2 箇所にほぼ集約

⸻

もし、
	•	具体的なパラメータ例（D, L, P, H など）ごとの FLOPs/トークンの見積り
	•	PyTorch での multi-layer NGRC block のスケッチ
	•	「多項式をこう制限するとこのくらいの M になる」といった設計案

みたいなところまで詰めたいなら、その前提で計算例付きで書きます。