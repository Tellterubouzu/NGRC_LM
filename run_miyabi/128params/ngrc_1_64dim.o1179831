[2025-12-16 22:28:03,903] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 22:28:06,101] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_222838-8ye5vbd7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(19.06M_d64_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-222806
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/8ye5vbd7
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=64, ngrc_lag=5, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=64, lag=5, poly_degree=3, phi_dim=1217, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 19.06M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.464676380157471, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 365974.3881623414, max_mem_mb: 20115.298816
[log] train loss: 5.947847843170166, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 375606.79091964517, max_mem_mb: 20115.298816
[log] train loss: 6.2727789878845215, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 373857.39049864904, max_mem_mb: 20115.29984
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 23.29it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:03, 24.33it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:03, 24.56it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 24.68it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 24.86it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 24.94it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 24.96it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:00<00:03, 24.99it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:02, 25.16it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:02, 25.31it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 25.40it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 25.49it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 25.57it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 25.61it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 25.65it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:01<00:02, 25.72it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:01, 25.76it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:01, 25.75it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 25.72it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 25.73it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 25.71it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 25.27it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:02<00:01, 25.06it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:02<00:01, 25.25it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:02<00:00, 25.34it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 25.40it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 25.47it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 25.45it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 25.45it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 25.48it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:03<00:00, 25.40it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:03<00:00, 25.42it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:03<00:00, 25.35it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:03<00:00, 25.33it/s]
wandb: updating run metadata; uploading artifact run-8ye5vbd7-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_60a7b7c0c13fa56ecb66.table.json
wandb: uploading artifact run-8ye5vbd7-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ
wandb:  grad_top/0_readout_out.weight ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÅ‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.40025
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.08433
wandb:  grad_top/0_readout_out.weight 0.42824
wandb: grad_top/0_readout_proj.weight 0.31991
wandb:  grad_top/1_readout_out.weight 0.23991
wandb: grad_top/1_readout_proj.weight 0.40545
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(19.06M_d64_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-222806 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/8ye5vbd7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_222838-8ye5vbd7/logs
Inference time: 3.95s, Tokens/sec: 51827.85, Memory: 3895.24MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(19.06M_d64_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-222806_report.txt
[2025-12-16 22:34:16,432] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 22:34:17,903] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_223452-u8hdvhs6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(19.55M_d64_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-223418
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/u8hdvhs6
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=64, ngrc_lag=10, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=64, lag=10, poly_degree=3, phi_dim=2177, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 19.55M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.487700939178467, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 342666.6856104146, max_mem_mb: 20279.926272
[log] train loss: 5.983265399932861, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 348976.1758122698, max_mem_mb: 20279.926272
[log] train loss: 6.278229236602783, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 358138.9516800984, max_mem_mb: 20279.927296
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 22.10it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 22.41it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.48it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.58it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.57it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.53it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.58it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.63it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.56it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.55it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.65it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.69it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.79it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.74it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 22.69it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.74it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.79it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.83it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.72it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.74it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.72it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.73it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.89it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 23.00it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.99it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.94it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.93it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.91it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.94it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 22.93it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.89it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.97it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.99it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.77it/s]
wandb: updating run metadata; uploading artifact run-u8hdvhs6-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_782a92d3b990d3494d5a.table.json
wandb: uploading artifact run-u8hdvhs6-mean_so_far_ppl_curve_table
wandb: uploading history steps 1953-1954, summary, console lines 6-8
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñà‚ñà‚ñÉ
wandb:  grad_top/0_readout_out.weight ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñà‚ñá‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÅ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñá‚ñà‚ñá‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.49158
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.08222
wandb:  grad_top/0_readout_out.weight 0.58178
wandb: grad_top/0_readout_proj.weight 0.4219
wandb:  grad_top/1_readout_out.weight 0.25151
wandb: grad_top/1_readout_proj.weight 0.57936
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(19.55M_d64_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-223418 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/u8hdvhs6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_223452-u8hdvhs6/logs
Inference time: 4.40s, Tokens/sec: 46583.18, Memory: 3902.26MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(19.55M_d64_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-223418_report.txt
[2025-12-16 22:40:40,515] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 22:40:42,050] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_224111-d6oofecf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(20.04M_d64_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251216-224042
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/d6oofecf
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=64, ngrc_lag=15, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=64, lag=15, poly_degree=3, phi_dim=3137, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 20.04M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.519843578338623, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 347681.0731149965, max_mem_mb: 20447.074816
[log] train loss: 6.0035881996154785, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 343007.42353419436, max_mem_mb: 20447.074816
[log] train loss: 6.2939772605896, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 345042.3947298728, max_mem_mb: 20447.07584
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 22.29it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 22.38it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.27it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.20it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.28it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.33it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.47it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.53it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.67it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.71it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.70it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.80it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.96it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 23.07it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 23.09it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 23.07it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 23.07it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:01, 23.08it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 23.12it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 23.29it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 23.27it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 23.17it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 23.15it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 23.00it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.93it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.94it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 23.00it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 23.05it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 23.10it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 23.12it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 23.06it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.98it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.88it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.88it/s]
wandb: updating run metadata; uploading artifact run-d6oofecf-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_ebfedb7ebae04e86fc02.table.json
wandb: uploading artifact run-d6oofecf-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñá‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÉ‚ñà‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÑ
wandb:  grad_top/0_readout_out.weight ‚ñÑ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÉ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.54644
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.08021
wandb:  grad_top/0_readout_out.weight 0.79097
wandb: grad_top/0_readout_proj.weight 0.48197
wandb:  grad_top/1_readout_out.weight 0.2565
wandb: grad_top/1_readout_proj.weight 0.73626
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(20.04M_d64_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251216-224042 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/d6oofecf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_224111-d6oofecf/logs
Inference time: 4.37s, Tokens/sec: 46820.01, Memory: 3911.56MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(20.04M_d64_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251216-224042_report.txt
[2025-12-16 22:47:06,778] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 22:47:08,256] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_224739-roe5h6ag
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(20.53M_d64_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251216-224708
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/roe5h6ag
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=64, ngrc_lag=20, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=64, lag=20, poly_degree=3, phi_dim=4097, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 20.53M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.533970355987549, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 343014.5574921342, max_mem_mb: 20614.845952
[log] train loss: 6.008743762969971, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 352358.5119792149, max_mem_mb: 20614.845952
[log] train loss: 6.294764041900635, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 355071.4519374693, max_mem_mb: 20614.846976
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.99it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 22.52it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.70it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.77it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.81it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.74it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.82it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.90it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.79it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.81it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.81it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.77it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.70it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.74it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 22.90it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.96it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.94it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.95it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 23.09it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 23.26it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 23.21it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 23.08it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 23.02it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.92it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.89it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.88it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.90it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.91it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.94it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 22.99it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.94it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 23.00it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 23.08it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.92it/s]
wandb: updating run metadata; uploading artifact run-roe5h6ag-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_9b962ac3925cb7cf9b66.table.json
wandb: uploading artifact run-roe5h6ag-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÑ‚ñÉ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÇ
wandb:  grad_top/0_readout_out.weight ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñà
wandb: grad_top/1_readout_proj.weight ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.64259
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.0783
wandb:  grad_top/0_readout_out.weight 0.75897
wandb: grad_top/0_readout_proj.weight 0.58154
wandb:  grad_top/1_readout_out.weight 0.27258
wandb: grad_top/1_readout_proj.weight 0.74295
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(20.53M_d64_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251216-224708 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/roe5h6ag
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_224739-roe5h6ag/logs
Inference time: 4.37s, Tokens/sec: 46890.14, Memory: 3921.42MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(20.53M_d64_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251216-224708_report.txt
[2025-12-16 22:53:31,231] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 22:53:32,816] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_225401-8y4l653n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(21.02M_d64_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251216-225333
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/8y4l653n
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=64, ngrc_lag=25, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=64, lag=25, poly_degree=3, phi_dim=5057, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 21.02M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.524142742156982, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 317887.43337754, max_mem_mb: 20780.293632
[log] train loss: 6.007674217224121, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 333988.12840850314, max_mem_mb: 20780.293632
[log] train loss: 6.305383682250977, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 340063.744493401, max_mem_mb: 20780.294656
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.95it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 22.07it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.21it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.25it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.23it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.21it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.08it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.01it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 21.89it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.09it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 22.23it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.41it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.49it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.59it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.53it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.59it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.65it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.68it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.79it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.84it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.70it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.65it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.64it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.68it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.74it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.72it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.74it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.73it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.75it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 22.78it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.84it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.80it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.75it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.54it/s]
wandb: updating run metadata; uploading artifact run-8y4l653n-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_c95a18188497e85ad247.table.json
wandb: uploading artifact run-8y4l653n-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:  grad_top/0_readout_out.weight ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ
wandb: grad_top/0_readout_proj.weight ‚ñá‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.64604
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.07644
wandb:  grad_top/0_readout_out.weight 0.80274
wandb: grad_top/0_readout_proj.weight 0.5854
wandb:  grad_top/1_readout_out.weight 0.27235
wandb: grad_top/1_readout_proj.weight 0.80077
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(21.02M_d64_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251216-225333 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/8y4l653n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_225401-8y4l653n/logs
Inference time: 4.44s, Tokens/sec: 46123.09, Memory: 3927.16MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(21.02M_d64_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251216-225333_report.txt
[2025-12-16 23:00:04,530] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:00:06,047] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_230039-4tz57kyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(21.51M_d64_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251216-230006
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/4tz57kyr
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=64, ngrc_lag=30, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=64, lag=30, poly_degree=3, phi_dim=6017, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 21.51M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.585337162017822, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 312304.9975651659, max_mem_mb: 20948.19584
[log] train loss: 6.049487113952637, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 317544.9888449256, max_mem_mb: 20948.19584
[log] train loss: 6.280166149139404, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 317507.931689444, max_mem_mb: 20948.196864
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.83it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 22.34it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.63it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.69it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.71it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.71it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.75it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.80it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.84it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.87it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.92it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.87it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.91it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.82it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 22.91it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.92it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.95it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:01, 23.02it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.93it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 23.00it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 23.06it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 23.12it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 23.16it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 23.14it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 23.21it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 23.14it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 23.14it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 23.20it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 23.14it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 23.23it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 23.17it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 23.22it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 23.20it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.98it/s]
wandb: updating run metadata; uploading artifact run-4tz57kyr-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_1697fe4b8646a4d863ae.table.json; uploading history steps 1953-1953, summary, console lines 6-7
wandb: uploading artifact run-4tz57kyr-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ
wandb:  grad_top/0_readout_out.weight ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:  grad_top/1_readout_out.weight ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.7044
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.0747
wandb:  grad_top/0_readout_out.weight 0.98239
wandb: grad_top/0_readout_proj.weight 0.64803
wandb:  grad_top/1_readout_out.weight 0.27535
wandb: grad_top/1_readout_proj.weight 0.96905
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(21.51M_d64_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251216-230006 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/4tz57kyr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_230039-4tz57kyr/logs
Inference time: 4.36s, Tokens/sec: 47022.26, Memory: 3935.35MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(21.51M_d64_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251216-230006_report.txt
[2025-12-16 23:07:07,839] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:07:09,403] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_230741-uuwqqed0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(21.59M_d128_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-230709
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/uuwqqed0
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=128, ngrc_lag=5, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=128, lag=5, poly_degree=3, phi_dim=2177, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 21.59M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.269677639007568, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 300195.9344507995, max_mem_mb: 20299.2256
[log] train loss: 5.693938255310059, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 309149.8746452854, max_mem_mb: 20299.2256
[log] train loss: 6.0472187995910645, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 308725.93420009775, max_mem_mb: 20299.226624
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 23.34it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:03, 23.86it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:03, 24.11it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 24.14it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 24.00it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 24.03it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 24.17it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:00<00:03, 24.33it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:02, 24.48it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:02, 24.61it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 24.66it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 24.72it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 24.74it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 24.67it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 24.77it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:01<00:02, 24.83it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:01, 24.78it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:01, 24.70it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 24.68it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 24.83it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 24.89it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 24.81it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:02<00:01, 24.86it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:02<00:01, 24.85it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 24.72it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 24.65it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 24.60it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 24.53it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 24.58it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 24.58it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:03<00:00, 24.53it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:03<00:00, 24.61it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 24.64it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.58it/s]
wandb: updating run metadata; uploading artifact run-uuwqqed0-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_c47507134174138bd75c.table.json
wandb: uploading artifact run-uuwqqed0-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ
wandb:  grad_top/0_readout_out.weight ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: grad_top/0_readout_proj.weight ‚ñà‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñà‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.50251
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.14884
wandb:  grad_top/0_readout_out.weight 0.44553
wandb: grad_top/0_readout_proj.weight 0.4207
wandb:  grad_top/1_readout_out.weight 0.27442
wandb: grad_top/1_readout_proj.weight 0.44231
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(21.59M_d128_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-230709 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/uuwqqed0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_230741-uuwqqed0/logs
Inference time: 4.07s, Tokens/sec: 50270.37, Memory: 3915.40MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(21.59M_d128_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-230709_report.txt
[2025-12-16 23:14:15,515] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:14:17,029] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_231446-cvbc65y0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(22.58M_d128_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-231417
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/cvbc65y0
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=128, ngrc_lag=10, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=128, lag=10, poly_degree=3, phi_dim=4097, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 22.58M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.311141490936279, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 306574.12130162655, max_mem_mb: 20634.963456
[log] train loss: 5.716814994812012, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 321564.77621350694, max_mem_mb: 20634.963456
[log] train loss: 6.046182632446289, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 329119.67325152596, max_mem_mb: 20634.96448
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.39it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.81it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 21.99it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:04, 21.99it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 21.96it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.17it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.26it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.33it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.36it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.46it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.49it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.43it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.30it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.37it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.31it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.39it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.51it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.47it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.44it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.48it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.46it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.37it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.43it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.42it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.47it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.37it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.57it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.62it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.70it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:04<00:00, 22.70it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.58it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.54it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.52it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.40it/s]
wandb: updating run metadata; uploading artifact run-cvbc65y0-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_591f134a5a7b965e7584.table.json
wandb: uploading artifact run-cvbc65y0-mean_so_far_ppl_curve_table
wandb: uploading data
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÜ‚ñÜ
wandb:  grad_top/0_readout_out.weight ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñà‚ñÅ‚ñà‚ñÇ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñá‚ñá‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñà‚ñá‚ñÖ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.62949
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.14237
wandb:  grad_top/0_readout_out.weight 0.76306
wandb: grad_top/0_readout_proj.weight 0.5593
wandb:  grad_top/1_readout_out.weight 0.28841
wandb: grad_top/1_readout_proj.weight 0.74403
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(22.58M_d128_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-231417 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/cvbc65y0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_231446-cvbc65y0/logs
Inference time: 4.47s, Tokens/sec: 45839.39, Memory: 3935.71MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(22.58M_d128_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-231417_report.txt
[2025-12-16 23:20:59,030] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:21:00,531] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_232136-spa0v9ol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(23.56M_d128_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251216-232100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/spa0v9ol
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=128, ngrc_lag=15, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=128, lag=15, poly_degree=3, phi_dim=6017, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 23.56M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.3132123947143555, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 301963.45062807965, max_mem_mb: 20966.317568
[log] train loss: 5.72929573059082, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 315037.7138296439, max_mem_mb: 20966.317568
[log] train loss: 6.046384334564209, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 310650.94018329686, max_mem_mb: 20966.318592
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.47it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.92it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.07it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.05it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.26it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.21it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.19it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.27it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.37it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.43it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.48it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.49it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.55it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.56it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.60it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.68it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.65it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.58it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.61it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.70it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.85it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.70it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.74it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.89it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.94it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.93it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.86it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.83it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.86it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 22.89it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.86it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.82it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.88it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.62it/s]
wandb: updating run metadata; uploading artifact run-spa0v9ol-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_729d2379aa8cc15ccf5c.table.json
wandb: uploading artifact run-spa0v9ol-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÜ
wandb:  grad_top/0_readout_out.weight ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ
wandb:  grad_top/1_readout_out.weight ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.68299
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.13641
wandb:  grad_top/0_readout_out.weight 1.10025
wandb: grad_top/0_readout_proj.weight 0.61264
wandb:  grad_top/1_readout_out.weight 0.30147
wandb: grad_top/1_readout_proj.weight 1.02412
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(23.56M_d128_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251216-232100 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/spa0v9ol
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_232136-spa0v9ol/logs
Inference time: 4.43s, Tokens/sec: 46277.98, Memory: 3947.97MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(23.56M_d128_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251216-232100_report.txt
[2025-12-16 23:28:08,790] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:28:10,285] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_232842-hh79oho5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(24.54M_d128_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251216-232810
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/hh79oho5
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=128, ngrc_lag=20, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=128, lag=20, poly_degree=3, phi_dim=7937, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 24.54M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.369791507720947, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 277647.9726020402, max_mem_mb: 21299.369472
[log] train loss: 5.755515098571777, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 298166.5444762301, max_mem_mb: 21299.369472
[log] train loss: 6.058813095092773, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 303951.89845437265, max_mem_mb: 21299.370496
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 20.70it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.25it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 21.35it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:04, 21.59it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 21.67it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 21.76it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 21.87it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 21.96it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 21.95it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 21.96it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 22.08it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.06it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.20it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.41it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.50it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.56it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.61it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.62it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.63it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.62it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.57it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.53it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.55it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.55it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.62it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.64it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.63it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.68it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.73it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:04<00:00, 22.76it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.74it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.67it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.74it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.36it/s]
wandb: updating run metadata; uploading artifact run-hh79oho5-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_f6608ad6ab2201494f4f.table.json
wandb: uploading artifact run-hh79oho5-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÉ
wandb:  grad_top/0_readout_out.weight ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ
wandb: grad_top/0_readout_proj.weight ‚ñà‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.77041
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.13098
wandb:  grad_top/0_readout_out.weight 1.98829
wandb: grad_top/0_readout_proj.weight 0.70262
wandb:  grad_top/1_readout_out.weight 0.31556
wandb: grad_top/1_readout_proj.weight 1.8052
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(24.54M_d128_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251216-232810 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/hh79oho5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_232842-hh79oho5/logs
Inference time: 4.48s, Tokens/sec: 45754.79, Memory: 3964.94MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(24.54M_d128_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251216-232810_report.txt
[2025-12-16 23:35:20,103] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:35:21,576] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_233553-w6vin4p4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(25.53M_d128_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251216-233522
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/w6vin4p4
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=128, ngrc_lag=25, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=128, lag=25, poly_degree=3, phi_dim=9857, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 25.53M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.365392208099365, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 271034.04367990897, max_mem_mb: 21632.425472
[log] train loss: 5.790770053863525, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 282579.08679086337, max_mem_mb: 21632.425472
[log] train loss: 6.075934410095215, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 283252.3702569555, max_mem_mb: 21632.426496
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.51it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.62it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 21.77it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.02it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.24it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.33it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.47it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.43it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.42it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.42it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.40it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.39it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.29it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.25it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.34it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.35it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.37it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.37it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.37it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.49it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.53it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.48it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.53it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.58it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.60it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.70it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.65it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.69it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.62it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:04<00:00, 22.58it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.56it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.45it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.55it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.42it/s]
wandb: updating run metadata; uploading artifact run-w6vin4p4-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_7fdad7c08e32578f0764.table.json
wandb: uploading artifact run-w6vin4p4-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÑ
wandb:  grad_top/0_readout_out.weight ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñà
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñà‚ñà‚ñá‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÖ‚ñà‚ñá‚ñá‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.79806
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.1259
wandb:  grad_top/0_readout_out.weight 4.00499
wandb: grad_top/0_readout_proj.weight 0.72571
wandb:  grad_top/1_readout_out.weight 0.3316
wandb: grad_top/1_readout_proj.weight 3.49783
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(25.53M_d128_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251216-233522 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/w6vin4p4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_233553-w6vin4p4/logs
Inference time: 4.46s, Tokens/sec: 45884.04, Memory: 3981.39MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(25.53M_d128_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251216-233522_report.txt
[2025-12-16 23:42:52,230] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:42:53,695] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_234324-mzjlvon4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(26.51M_d128_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251216-234254
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/mzjlvon4
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=128, ngrc_lag=30, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=128, lag=30, poly_degree=3, phi_dim=11777, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 26.51M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.398764610290527, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 270894.10443657154, max_mem_mb: 21968.820736
[log] train loss: 5.787489414215088, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 275611.1539706653, max_mem_mb: 21968.820736
[log] train loss: 6.065273761749268, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 270881.0324896886, max_mem_mb: 21968.82176
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 20.99it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.70it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.05it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.15it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.23it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.36it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.50it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.61it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.57it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.66it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.79it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.86it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.93it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.96it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:01<00:02, 23.00it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 23.09it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 23.11it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:01, 23.05it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 23.07it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 23.05it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 23.00it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.88it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.82it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.95it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.98it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.96it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.99it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.98it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 23.02it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 23.10it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.98it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.92it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 23.00it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.82it/s]
wandb: updating run metadata; uploading artifact run-mzjlvon4-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_aee50d195dfb48884289.table.json
wandb: uploading artifact run-mzjlvon4-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÖ
wandb:  grad_top/0_readout_out.weight ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà
wandb: grad_top/0_readout_proj.weight ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñá‚ñá‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.86437
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.12123
wandb:  grad_top/0_readout_out.weight 4.17594
wandb: grad_top/0_readout_proj.weight 0.79348
wandb:  grad_top/1_readout_out.weight 0.34237
wandb: grad_top/1_readout_proj.weight 4.15881
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(26.51M_d128_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251216-234254 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/mzjlvon4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_234324-mzjlvon4/logs
Inference time: 4.39s, Tokens/sec: 46697.41, Memory: 3997.84MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(26.51M_d128_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251216-234254_report.txt
[2025-12-16 23:50:45,398] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:50:46,870] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_235121-dsthwbkq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(26.67M_d256_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-235047
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/dsthwbkq
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=256, ngrc_lag=5, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=256, lag=5, poly_degree=3, phi_dim=4097, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 26.67M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.097567558288574, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 268408.61782230396, max_mem_mb: 20671.471104
[log] train loss: 5.446544170379639, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 283087.271871182, max_mem_mb: 20671.471104
[log] train loss: 5.833069801330566, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 285669.2936250787, max_mem_mb: 20671.472128
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.70it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 22.05it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.23it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.04it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.14it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.14it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.23it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.31it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.42it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.53it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.54it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.65it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.75it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.87it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.82it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.95it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 23.00it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.97it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.94it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.84it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.98it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.83it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.89it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.86it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.74it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.65it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.75it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.91it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.92it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 22.97it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.91it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.85it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.81it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.69it/s]
wandb: updating run metadata; uploading artifact run-dsthwbkq-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_3218af61cb7808ca3bd8.table.json
wandb: uploading artifact run-dsthwbkq-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñÇ
wandb:  grad_top/0_readout_out.weight ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñà‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÑ‚ñà‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÉ
wandb: grad_top/1_readout_proj.weight ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÜ‚ñÖ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.68712
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.24101
wandb:  grad_top/0_readout_out.weight 0.87243
wandb: grad_top/0_readout_proj.weight 0.61041
wandb:  grad_top/1_readout_out.weight 0.31522
wandb: grad_top/1_readout_proj.weight 0.85173
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(26.67M_d256_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-235047 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/dsthwbkq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_235121-dsthwbkq/logs
Inference time: 4.41s, Tokens/sec: 46429.11, Memory: 3959.83MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(26.67M_d256_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251216-235047_report.txt
[2025-12-16 23:58:21,799] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 23:58:23,248] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251216_235854-xuoqbpbl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(28.64M_d256_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-235823
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/xuoqbpbl
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=256, ngrc_lag=10, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=256, lag=10, poly_degree=3, phi_dim=7937, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 28.64M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.1481475830078125, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 257904.4147295784, max_mem_mb: 21338.494464
[log] train loss: 5.484033584594727, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 273156.41872629203, max_mem_mb: 21338.88768
[log] train loss: 5.81403923034668, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 276140.54510938434, max_mem_mb: 21338.888704
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   2%|‚ñè         | 2/100 [00:00<00:06, 16.14it/s]Computing mean-so-far PPL:   5%|‚ñå         | 5/100 [00:00<00:04, 19.99it/s]Computing mean-so-far PPL:   8%|‚ñä         | 8/100 [00:00<00:04, 21.35it/s]Computing mean-so-far PPL:  11%|‚ñà         | 11/100 [00:00<00:04, 22.10it/s]Computing mean-so-far PPL:  14%|‚ñà‚ñç        | 14/100 [00:00<00:03, 22.49it/s]Computing mean-so-far PPL:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 22.70it/s]Computing mean-so-far PPL:  20%|‚ñà‚ñà        | 20/100 [00:00<00:03, 22.86it/s]Computing mean-so-far PPL:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:01<00:03, 22.89it/s]Computing mean-so-far PPL:  26%|‚ñà‚ñà‚ñå       | 26/100 [00:01<00:03, 23.01it/s]Computing mean-so-far PPL:  29%|‚ñà‚ñà‚ñâ       | 29/100 [00:01<00:03, 23.02it/s]Computing mean-so-far PPL:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:01<00:02, 23.01it/s]Computing mean-so-far PPL:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:01<00:02, 23.02it/s]Computing mean-so-far PPL:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:01<00:02, 23.05it/s]Computing mean-so-far PPL:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:02, 23.20it/s]Computing mean-so-far PPL:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:01<00:02, 23.31it/s]Computing mean-so-far PPL:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:02<00:02, 23.30it/s]Computing mean-so-far PPL:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:02<00:02, 23.44it/s]Computing mean-so-far PPL:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:02<00:02, 23.46it/s]Computing mean-so-far PPL:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:02<00:01, 23.47it/s]Computing mean-so-far PPL:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:02<00:01, 23.52it/s]Computing mean-so-far PPL:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:02<00:01, 23.56it/s]Computing mean-so-far PPL:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 23.45it/s]Computing mean-so-far PPL:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:02<00:01, 23.49it/s]Computing mean-so-far PPL:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:03<00:01, 23.54it/s]Computing mean-so-far PPL:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:03<00:01, 23.62it/s]Computing mean-so-far PPL:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:03<00:00, 23.58it/s]Computing mean-so-far PPL:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:03<00:00, 23.59it/s]Computing mean-so-far PPL:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:03<00:00, 23.56it/s]Computing mean-so-far PPL:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:03<00:00, 23.58it/s]Computing mean-so-far PPL:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:03<00:00, 23.59it/s]Computing mean-so-far PPL:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:03<00:00, 23.68it/s]Computing mean-so-far PPL:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:04<00:00, 23.53it/s]Computing mean-so-far PPL:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:04<00:00, 23.49it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.15it/s]
wandb: updating run metadata; uploading artifact run-xuoqbpbl-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_ee9b0cef4e3be0b7f936.table.json
wandb: uploading artifact run-xuoqbpbl-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÑ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÉ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÅ
wandb:  grad_top/0_readout_out.weight ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñÖ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.82439
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.22448
wandb:  grad_top/0_readout_out.weight 1.48164
wandb: grad_top/0_readout_proj.weight 0.7509
wandb:  grad_top/1_readout_out.weight 0.33997
wandb: grad_top/1_readout_proj.weight 1.43271
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(28.64M_d256_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-235823 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/xuoqbpbl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251216_235854-xuoqbpbl/logs
Inference time: 4.32s, Tokens/sec: 47362.37, Memory: 3991.88MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(28.64M_d256_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251216-235823_report.txt
[2025-12-17 00:06:05,535] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 00:06:07,008] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_000642-4t3dkg8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(30.61M_d256_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251217-000607
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/4t3dkg8n
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=256, ngrc_lag=15, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=256, lag=15, poly_degree=3, phi_dim=11777, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 30.61M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.17861270904541, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 226288.0258663967, max_mem_mb: 22005.784064
[log] train loss: 5.513179302215576, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 234089.8765160449, max_mem_mb: 22005.784064
[log] train loss: 5.833246231079102, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 234630.32865494533, max_mem_mb: 22005.785088
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   2%|‚ñè         | 2/100 [00:00<00:05, 18.67it/s]Computing mean-so-far PPL:   5%|‚ñå         | 5/100 [00:00<00:04, 21.37it/s]Computing mean-so-far PPL:   8%|‚ñä         | 8/100 [00:00<00:04, 22.26it/s]Computing mean-so-far PPL:  11%|‚ñà         | 11/100 [00:00<00:03, 22.63it/s]Computing mean-so-far PPL:  14%|‚ñà‚ñç        | 14/100 [00:00<00:03, 22.88it/s]Computing mean-so-far PPL:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 23.05it/s]Computing mean-so-far PPL:  20%|‚ñà‚ñà        | 20/100 [00:00<00:03, 23.14it/s]Computing mean-so-far PPL:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:01<00:03, 23.27it/s]Computing mean-so-far PPL:  26%|‚ñà‚ñà‚ñå       | 26/100 [00:01<00:03, 23.38it/s]Computing mean-so-far PPL:  29%|‚ñà‚ñà‚ñâ       | 29/100 [00:01<00:03, 23.46it/s]Computing mean-so-far PPL:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:01<00:02, 23.45it/s]Computing mean-so-far PPL:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:01<00:02, 23.54it/s]Computing mean-so-far PPL:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:01<00:02, 23.50it/s]Computing mean-so-far PPL:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:02, 23.47it/s]Computing mean-so-far PPL:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:01<00:02, 23.46it/s]Computing mean-so-far PPL:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:02<00:02, 23.49it/s]Computing mean-so-far PPL:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:02<00:02, 23.48it/s]Computing mean-so-far PPL:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:02<00:02, 23.45it/s]Computing mean-so-far PPL:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:02<00:01, 23.44it/s]Computing mean-so-far PPL:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:02<00:01, 23.42it/s]Computing mean-so-far PPL:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:02<00:01, 23.44it/s]Computing mean-so-far PPL:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 23.50it/s]Computing mean-so-far PPL:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:02<00:01, 23.52it/s]Computing mean-so-far PPL:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:03<00:01, 23.54it/s]Computing mean-so-far PPL:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:03<00:01, 23.46it/s]Computing mean-so-far PPL:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:03<00:00, 23.40it/s]Computing mean-so-far PPL:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:03<00:00, 23.48it/s]Computing mean-so-far PPL:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:03<00:00, 23.55it/s]Computing mean-so-far PPL:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:03<00:00, 23.59it/s]Computing mean-so-far PPL:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:03<00:00, 23.69it/s]Computing mean-so-far PPL:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:03<00:00, 23.59it/s]Computing mean-so-far PPL:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:04<00:00, 23.47it/s]Computing mean-so-far PPL:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:04<00:00, 23.56it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.35it/s]
wandb: updating run metadata; uploading artifact run-4t3dkg8n-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_ea484c6d9b45335f9a4b.table.json
wandb: uploading artifact run-4t3dkg8n-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñá‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñá‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ
wandb:  grad_top/0_readout_out.weight ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñà
wandb: grad_top/0_readout_proj.weight ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñá‚ñÉ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÉ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.92378
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.21002
wandb:  grad_top/0_readout_out.weight 4.6414
wandb: grad_top/0_readout_proj.weight 0.84739
wandb:  grad_top/1_readout_out.weight 0.36753
wandb: grad_top/1_readout_proj.weight 4.5152
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(30.61M_d256_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251217-000607 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/4t3dkg8n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_000642-4t3dkg8n/logs
Inference time: 4.29s, Tokens/sec: 47762.42, Memory: 4023.27MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(30.61M_d256_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251217-000607_report.txt
[2025-12-17 00:14:52,385] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 00:14:53,871] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_001531-2eadulxt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(32.57M_d256_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251217-001454
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/2eadulxt
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=256, ngrc_lag=20, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=256, lag=20, poly_degree=3, phi_dim=15617, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 32.57M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.264278888702393, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 217723.12941604544, max_mem_mb: 22671.891968
[log] train loss: 5.561945915222168, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 232243.7904872615, max_mem_mb: 22671.891968
[log] train loss: 5.8340044021606445, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 227121.9160632302, max_mem_mb: 22671.892992
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.04it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.62it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 21.88it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:04, 21.77it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 21.87it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 21.90it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.00it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.04it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.03it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.15it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 22.07it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.14it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.21it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.24it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.32it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.39it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.46it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.35it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.26it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.28it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.31it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.28it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.33it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.28it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.33it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.34it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.42it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.42it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.41it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:04<00:00, 22.39it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.37it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.40it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.43it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.23it/s]
wandb: updating run metadata; uploading artifact run-2eadulxt-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_b0d88ef319bb0e7be8ed.table.json
wandb: uploading artifact run-2eadulxt-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñÅ
wandb:  grad_top/0_readout_out.weight ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñà
wandb: grad_top/0_readout_proj.weight ‚ñà‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñà
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.94735
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.19734
wandb:  grad_top/0_readout_out.weight 4.68561
wandb: grad_top/0_readout_proj.weight 0.86093
wandb:  grad_top/1_readout_out.weight 0.39503
wandb: grad_top/1_readout_proj.weight 4.47876
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(32.57M_d256_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251217-001454 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/2eadulxt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_001531-2eadulxt/logs
Inference time: 4.50s, Tokens/sec: 45473.21, Memory: 4055.91MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(32.57M_d256_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251217-001454_report.txt
[2025-12-17 00:24:09,152] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 00:24:10,607] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_002443-gcm4xn3n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(34.54M_d256_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251217-002411
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/gcm4xn3n
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=256, ngrc_lag=25, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=256, lag=25, poly_degree=3, phi_dim=19457, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 34.54M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.265009880065918, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 202760.0740422169, max_mem_mb: 23340.359168
[log] train loss: 5.577208518981934, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 210723.7502089689, max_mem_mb: 23340.359168
[log] train loss: 5.847318649291992, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 207570.35163729213, max_mem_mb: 23340.360192
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 21.44it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.76it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 22.05it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:03, 22.21it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 22.15it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 22.25it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 22.29it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 22.35it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 22.32it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 22.51it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:02, 22.58it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 22.65it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.68it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 22.65it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.75it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.71it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.76it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.70it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.67it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.61it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.68it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:02<00:01, 22.67it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.72it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.73it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.72it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.58it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.62it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.58it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.59it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:03<00:00, 22.55it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.54it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.59it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.50it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.53it/s]
wandb: updating run metadata; uploading artifact run-gcm4xn3n-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_c3034640efe66820fe4b.table.json
wandb: uploading artifact run-gcm4xn3n-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñÅ‚ñà‚ñÑ
wandb:  grad_top/0_readout_out.weight ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÉ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñà‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÉ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.99904
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.18611
wandb:  grad_top/0_readout_out.weight 6.23798
wandb: grad_top/0_readout_proj.weight 0.90431
wandb:  grad_top/1_readout_out.weight 0.42437
wandb: grad_top/1_readout_proj.weight 5.71932
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(34.54M_d256_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251217-002411 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/gcm4xn3n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_002443-gcm4xn3n/logs
Inference time: 4.44s, Tokens/sec: 46095.01, Memory: 4086.97MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(34.54M_d256_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251217-002411_report.txt
[2025-12-17 00:33:56,137] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 00:33:57,606] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_003432-ifm6kw1j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(36.50M_d256_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251217-003358
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/ifm6kw1j
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=256, ngrc_lag=30, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=256, lag=30, poly_degree=3, phi_dim=23297, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 36.50M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.3270158767700195, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 189220.0118141152, max_mem_mb: 24005.418496
[log] train loss: 5.610032081604004, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 195877.26738454017, max_mem_mb: 24005.418496
[log] train loss: 5.847851753234863, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 192492.30423203725, max_mem_mb: 24005.41952
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   1%|          | 1/100 [00:00<00:10,  9.68it/s]Computing mean-so-far PPL:   4%|‚ñç         | 4/100 [00:00<00:05, 18.06it/s]Computing mean-so-far PPL:   7%|‚ñã         | 7/100 [00:00<00:04, 20.17it/s]Computing mean-so-far PPL:  10%|‚ñà         | 10/100 [00:00<00:04, 20.97it/s]Computing mean-so-far PPL:  13%|‚ñà‚ñé        | 13/100 [00:00<00:04, 21.47it/s]Computing mean-so-far PPL:  16%|‚ñà‚ñå        | 16/100 [00:00<00:03, 21.73it/s]Computing mean-so-far PPL:  19%|‚ñà‚ñâ        | 19/100 [00:00<00:03, 21.75it/s]Computing mean-so-far PPL:  22%|‚ñà‚ñà‚ñè       | 22/100 [00:01<00:03, 21.86it/s]Computing mean-so-far PPL:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:03, 21.94it/s]Computing mean-so-far PPL:  28%|‚ñà‚ñà‚ñä       | 28/100 [00:01<00:03, 21.90it/s]Computing mean-so-far PPL:  31%|‚ñà‚ñà‚ñà       | 31/100 [00:01<00:03, 21.88it/s]Computing mean-so-far PPL:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [00:01<00:03, 21.87it/s]Computing mean-so-far PPL:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [00:01<00:02, 21.81it/s]Computing mean-so-far PPL:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:01<00:02, 21.79it/s]Computing mean-so-far PPL:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [00:02<00:02, 21.73it/s]Computing mean-so-far PPL:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [00:02<00:02, 21.70it/s]Computing mean-so-far PPL:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 21.74it/s]Computing mean-so-far PPL:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [00:02<00:02, 21.80it/s]Computing mean-so-far PPL:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [00:02<00:02, 21.89it/s]Computing mean-so-far PPL:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [00:02<00:01, 21.94it/s]Computing mean-so-far PPL:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [00:02<00:01, 21.92it/s]Computing mean-so-far PPL:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [00:02<00:01, 21.97it/s]Computing mean-so-far PPL:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [00:03<00:01, 21.88it/s]Computing mean-so-far PPL:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [00:03<00:01, 21.84it/s]Computing mean-so-far PPL:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 21.86it/s]Computing mean-so-far PPL:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [00:03<00:01, 21.88it/s]Computing mean-so-far PPL:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [00:03<00:00, 21.88it/s]Computing mean-so-far PPL:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [00:03<00:00, 21.86it/s]Computing mean-so-far PPL:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [00:03<00:00, 21.85it/s]Computing mean-so-far PPL:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [00:04<00:00, 21.83it/s]Computing mean-so-far PPL:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [00:04<00:00, 21.79it/s]Computing mean-so-far PPL:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [00:04<00:00, 21.70it/s]Computing mean-so-far PPL:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 21.61it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 21.56it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 21.61it/s]
wandb: updating run metadata; uploading artifact run-ifm6kw1j-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_30b84eb19acc89ae2d79.table.json
wandb: uploading artifact run-ifm6kw1j-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:  grad_top/0_readout_out.weight ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñà‚ñÑ‚ñÉ
wandb: grad_top/0_readout_proj.weight ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñá‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñà‚ñÑ‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 1.05394
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.17608
wandb:  grad_top/0_readout_out.weight 5.63421
wandb: grad_top/0_readout_proj.weight 0.95872
wandb:  grad_top/1_readout_out.weight 0.4375
wandb: grad_top/1_readout_proj.weight 4.99363
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(36.50M_d256_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251217-003358 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/ifm6kw1j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_003432-ifm6kw1j/logs
Inference time: 4.64s, Tokens/sec: 44184.80, Memory: 4120.66MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(36.50M_d256_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251217-003358_report.txt
[2025-12-17 00:44:29,192] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 00:44:31,095] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_004500-k8pl2vmx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(36.83M_d512_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251217-004431
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/k8pl2vmx
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=512, ngrc_lag=5, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=512, lag=5, poly_degree=3, phi_dim=7937, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 36.83M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 5.944931983947754, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 215645.35935515724, max_mem_mb: 21413.860864
[log] train loss: 5.270312309265137, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 221904.5559486043, max_mem_mb: 21413.860864
[log] train loss: 5.625649452209473, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 222574.40709930958, max_mem_mb: 21413.861888
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   3%|‚ñé         | 3/100 [00:00<00:04, 20.77it/s]Computing mean-so-far PPL:   6%|‚ñå         | 6/100 [00:00<00:04, 21.57it/s]Computing mean-so-far PPL:   9%|‚ñâ         | 9/100 [00:00<00:04, 21.77it/s]Computing mean-so-far PPL:  12%|‚ñà‚ñè        | 12/100 [00:00<00:04, 21.81it/s]Computing mean-so-far PPL:  15%|‚ñà‚ñå        | 15/100 [00:00<00:03, 21.83it/s]Computing mean-so-far PPL:  18%|‚ñà‚ñä        | 18/100 [00:00<00:03, 21.81it/s]Computing mean-so-far PPL:  21%|‚ñà‚ñà        | 21/100 [00:00<00:03, 21.91it/s]Computing mean-so-far PPL:  24%|‚ñà‚ñà‚ñç       | 24/100 [00:01<00:03, 21.93it/s]Computing mean-so-far PPL:  27%|‚ñà‚ñà‚ñã       | 27/100 [00:01<00:03, 21.86it/s]Computing mean-so-far PPL:  30%|‚ñà‚ñà‚ñà       | 30/100 [00:01<00:03, 21.89it/s]Computing mean-so-far PPL:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [00:01<00:03, 21.93it/s]Computing mean-so-far PPL:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:01<00:02, 21.92it/s]Computing mean-so-far PPL:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [00:01<00:02, 22.02it/s]Computing mean-so-far PPL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [00:01<00:02, 21.97it/s]Computing mean-so-far PPL:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [00:02<00:02, 22.02it/s]Computing mean-so-far PPL:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:02<00:02, 22.07it/s]Computing mean-so-far PPL:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [00:02<00:02, 22.08it/s]Computing mean-so-far PPL:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [00:02<00:02, 22.09it/s]Computing mean-so-far PPL:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:02<00:01, 22.13it/s]Computing mean-so-far PPL:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:02<00:01, 22.18it/s]Computing mean-so-far PPL:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [00:02<00:01, 22.20it/s]Computing mean-so-far PPL:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [00:03<00:01, 22.15it/s]Computing mean-so-far PPL:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [00:03<00:01, 22.14it/s]Computing mean-so-far PPL:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:03<00:01, 22.26it/s]Computing mean-so-far PPL:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:03<00:01, 22.25it/s]Computing mean-so-far PPL:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [00:03<00:00, 22.22it/s]Computing mean-so-far PPL:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [00:03<00:00, 22.29it/s]Computing mean-so-far PPL:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [00:03<00:00, 22.29it/s]Computing mean-so-far PPL:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:03<00:00, 22.31it/s]Computing mean-so-far PPL:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [00:04<00:00, 22.33it/s]Computing mean-so-far PPL:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:04<00:00, 22.21it/s]Computing mean-so-far PPL:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:04<00:00, 22.25it/s]Computing mean-so-far PPL:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [00:04<00:00, 22.27it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.08it/s]
wandb: updating run metadata; uploading artifact run-k8pl2vmx-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_a9b1d5998c6ca533661b.table.json
wandb: uploading artifact run-k8pl2vmx-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:  grad_top/0_readout_out.weight ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ
wandb: grad_top/0_readout_proj.weight ‚ñà‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.85961
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.34907
wandb:  grad_top/0_readout_out.weight 1.52912
wandb: grad_top/0_readout_proj.weight 0.77972
wandb:  grad_top/1_readout_out.weight 0.36167
wandb: grad_top/1_readout_proj.weight 1.51357
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(36.83M_d512_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251217-004431 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/k8pl2vmx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_004500-k8pl2vmx/logs
Inference time: 4.53s, Tokens/sec: 45172.97, Memory: 4041.03MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(36.83M_d512_lag5_poly3_rank512_lr0.0005_bs200_seq256_20251217-004431_report.txt
[2025-12-17 00:53:38,293] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 00:53:39,726] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_005423-cd65blup
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(40.76M_d512_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251217-005340
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/cd65blup
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=512, ngrc_lag=10, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=512, lag=10, poly_degree=3, phi_dim=15617, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 40.76M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.036055088043213, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 178678.00694814877, max_mem_mb: 22747.38944
[log] train loss: 5.292118072509766, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 188432.17006417573, max_mem_mb: 22747.38944
[log] train loss: 5.623954772949219, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 190779.16834106212, max_mem_mb: 22747.390464
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   2%|‚ñè         | 2/100 [00:00<00:06, 15.04it/s]Computing mean-so-far PPL:   5%|‚ñå         | 5/100 [00:00<00:05, 18.82it/s]Computing mean-so-far PPL:   8%|‚ñä         | 8/100 [00:00<00:04, 20.28it/s]Computing mean-so-far PPL:  11%|‚ñà         | 11/100 [00:00<00:04, 20.73it/s]Computing mean-so-far PPL:  14%|‚ñà‚ñç        | 14/100 [00:00<00:04, 21.00it/s]Computing mean-so-far PPL:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 21.14it/s]Computing mean-so-far PPL:  20%|‚ñà‚ñà        | 20/100 [00:00<00:03, 21.13it/s]Computing mean-so-far PPL:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:01<00:03, 21.05it/s]Computing mean-so-far PPL:  26%|‚ñà‚ñà‚ñå       | 26/100 [00:01<00:03, 21.13it/s]Computing mean-so-far PPL:  29%|‚ñà‚ñà‚ñâ       | 29/100 [00:01<00:03, 21.28it/s]Computing mean-so-far PPL:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:01<00:03, 21.35it/s]Computing mean-so-far PPL:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:01<00:03, 21.42it/s]Computing mean-so-far PPL:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:01<00:02, 21.47it/s]Computing mean-so-far PPL:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:02, 21.52it/s]Computing mean-so-far PPL:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:02<00:02, 21.55it/s]Computing mean-so-far PPL:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:02<00:02, 21.69it/s]Computing mean-so-far PPL:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:02<00:02, 21.66it/s]Computing mean-so-far PPL:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:02<00:02, 21.71it/s]Computing mean-so-far PPL:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:02<00:02, 21.77it/s]Computing mean-so-far PPL:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:02<00:01, 21.81it/s]Computing mean-so-far PPL:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:02<00:01, 21.81it/s]Computing mean-so-far PPL:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 21.73it/s]Computing mean-so-far PPL:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:03<00:01, 21.69it/s]Computing mean-so-far PPL:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:03<00:01, 21.67it/s]Computing mean-so-far PPL:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:03<00:01, 21.71it/s]Computing mean-so-far PPL:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:03<00:01, 21.69it/s]Computing mean-so-far PPL:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:03<00:00, 21.63it/s]Computing mean-so-far PPL:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:03<00:00, 21.66it/s]Computing mean-so-far PPL:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:04<00:00, 21.67it/s]Computing mean-so-far PPL:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 21.61it/s]Computing mean-so-far PPL:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:04<00:00, 21.63it/s]Computing mean-so-far PPL:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:04<00:00, 21.60it/s]Computing mean-so-far PPL:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:04<00:00, 21.57it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 21.40it/s]
wandb: updating run metadata; uploading artifact run-cd65blup-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_ff5b209a5a7472b22cd8.table.json
wandb: uploading artifact run-cd65blup-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñà‚ñá‚ñÑ‚ñà
wandb:  grad_top/0_readout_out.weight ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñà
wandb: grad_top/0_readout_proj.weight ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñá‚ñà‚ñá‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ
wandb: grad_top/1_readout_proj.weight ‚ñá‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñá
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 0.98917
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.31537
wandb:  grad_top/0_readout_out.weight 2.52509
wandb: grad_top/0_readout_proj.weight 0.90044
wandb:  grad_top/1_readout_out.weight 0.40927
wandb: grad_top/1_readout_proj.weight 2.32693
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(40.76M_d512_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251217-005340 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/cd65blup
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_005423-cd65blup/logs
Inference time: 4.68s, Tokens/sec: 43764.18, Memory: 4106.50MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(40.76M_d512_lag10_poly3_rank512_lr0.0005_bs200_seq256_20251217-005340_report.txt
[2025-12-17 01:04:18,223] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 01:04:20,118] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_010451-fhk8zzeq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(44.70M_d512_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251217-010420
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/fhk8zzeq
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=512, ngrc_lag=15, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=512, lag=15, poly_degree=3, phi_dim=23297, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 44.70M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.154826641082764, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 160038.09524405713, max_mem_mb: 24080.915968
[log] train loss: 5.334531784057617, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 167666.8670413544, max_mem_mb: 24081.43872
[log] train loss: 5.648947715759277, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 166282.77385514064, max_mem_mb: 24081.439744
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   1%|          | 1/100 [00:00<00:17,  5.78it/s]Computing mean-so-far PPL:   4%|‚ñç         | 4/100 [00:00<00:06, 14.91it/s]Computing mean-so-far PPL:   7%|‚ñã         | 7/100 [00:00<00:05, 18.40it/s]Computing mean-so-far PPL:  10%|‚ñà         | 10/100 [00:00<00:04, 20.39it/s]Computing mean-so-far PPL:  13%|‚ñà‚ñé        | 13/100 [00:00<00:04, 21.52it/s]Computing mean-so-far PPL:  16%|‚ñà‚ñå        | 16/100 [00:00<00:03, 22.16it/s]Computing mean-so-far PPL:  19%|‚ñà‚ñâ        | 19/100 [00:00<00:03, 22.58it/s]Computing mean-so-far PPL:  22%|‚ñà‚ñà‚ñè       | 22/100 [00:01<00:03, 22.79it/s]Computing mean-so-far PPL:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:01<00:03, 23.09it/s]Computing mean-so-far PPL:  28%|‚ñà‚ñà‚ñä       | 28/100 [00:01<00:03, 23.27it/s]Computing mean-so-far PPL:  31%|‚ñà‚ñà‚ñà       | 31/100 [00:01<00:02, 23.33it/s]Computing mean-so-far PPL:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [00:01<00:02, 23.42it/s]Computing mean-so-far PPL:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [00:01<00:02, 23.41it/s]Computing mean-so-far PPL:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:01<00:02, 23.57it/s]Computing mean-so-far PPL:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [00:01<00:02, 23.62it/s]Computing mean-so-far PPL:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [00:02<00:02, 23.72it/s]Computing mean-so-far PPL:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [00:02<00:02, 23.89it/s]Computing mean-so-far PPL:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [00:02<00:02, 23.87it/s]Computing mean-so-far PPL:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [00:02<00:01, 23.83it/s]Computing mean-so-far PPL:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [00:02<00:01, 23.69it/s]Computing mean-so-far PPL:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [00:02<00:01, 23.90it/s]Computing mean-so-far PPL:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [00:02<00:01, 23.98it/s]Computing mean-so-far PPL:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [00:02<00:01, 24.03it/s]Computing mean-so-far PPL:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [00:03<00:01, 24.01it/s]Computing mean-so-far PPL:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [00:03<00:01, 23.99it/s]Computing mean-so-far PPL:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [00:03<00:01, 23.88it/s]Computing mean-so-far PPL:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [00:03<00:00, 23.69it/s]Computing mean-so-far PPL:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [00:03<00:00, 23.59it/s]Computing mean-so-far PPL:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [00:03<00:00, 23.57it/s]Computing mean-so-far PPL:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [00:03<00:00, 23.68it/s]Computing mean-so-far PPL:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [00:03<00:00, 23.79it/s]Computing mean-so-far PPL:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [00:04<00:00, 23.83it/s]Computing mean-so-far PPL:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [00:04<00:00, 23.90it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.98it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.02it/s]
wandb: updating run metadata; uploading artifact run-fhk8zzeq-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_161eaf6189bc9faa7e36.table.json
wandb: uploading artifact run-fhk8zzeq-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:  grad_top/0_readout_out.weight ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñÇ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 1.06761
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.28763
wandb:  grad_top/0_readout_out.weight 4.5831
wandb: grad_top/0_readout_proj.weight 0.96958
wandb:  grad_top/1_readout_out.weight 0.44667
wandb: grad_top/1_readout_proj.weight 4.56636
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(44.70M_d512_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251217-010420 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/fhk8zzeq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_010451-fhk8zzeq/logs
Inference time: 4.35s, Tokens/sec: 47046.08, Memory: 4171.64MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(44.70M_d512_lag15_poly3_rank512_lr0.0005_bs200_seq256_20251217-010420_report.txt
[2025-12-17 01:16:02,769] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 01:16:04,679] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_011639-en8cmu4o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(48.63M_d512_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251217-011605
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/en8cmu4o
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=512, ngrc_lag=20, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=512, lag=20, poly_degree=3, phi_dim=30977, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 48.63M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.234832286834717, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 155848.3097974419, max_mem_mb: 25415.491072
[log] train loss: 5.3923563957214355, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 157711.21629964258, max_mem_mb: 25416.014848
[log] train loss: 5.655879497528076, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 155705.21227685365, max_mem_mb: 25416.015872
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   2%|‚ñè         | 2/100 [00:00<00:05, 16.50it/s]Computing mean-so-far PPL:   5%|‚ñå         | 5/100 [00:00<00:04, 20.03it/s]Computing mean-so-far PPL:   8%|‚ñä         | 8/100 [00:00<00:04, 21.04it/s]Computing mean-so-far PPL:  11%|‚ñà         | 11/100 [00:00<00:04, 21.53it/s]Computing mean-so-far PPL:  14%|‚ñà‚ñç        | 14/100 [00:00<00:04, 21.50it/s]Computing mean-so-far PPL:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 21.61it/s]Computing mean-so-far PPL:  20%|‚ñà‚ñà        | 20/100 [00:00<00:03, 21.83it/s]Computing mean-so-far PPL:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:01<00:03, 21.88it/s]Computing mean-so-far PPL:  26%|‚ñà‚ñà‚ñå       | 26/100 [00:01<00:03, 22.05it/s]Computing mean-so-far PPL:  29%|‚ñà‚ñà‚ñâ       | 29/100 [00:01<00:03, 22.05it/s]Computing mean-so-far PPL:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:01<00:03, 21.98it/s]Computing mean-so-far PPL:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:01<00:02, 21.92it/s]Computing mean-so-far PPL:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:01<00:02, 21.84it/s]Computing mean-so-far PPL:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:02, 21.82it/s]Computing mean-so-far PPL:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:02<00:02, 21.93it/s]Computing mean-so-far PPL:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:02<00:02, 22.05it/s]Computing mean-so-far PPL:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:02<00:02, 22.11it/s]Computing mean-so-far PPL:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:02<00:02, 22.22it/s]Computing mean-so-far PPL:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:02<00:01, 22.24it/s]Computing mean-so-far PPL:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:02<00:01, 22.27it/s]Computing mean-so-far PPL:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:02<00:01, 22.32it/s]Computing mean-so-far PPL:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 22.40it/s]Computing mean-so-far PPL:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:03<00:01, 22.41it/s]Computing mean-so-far PPL:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:03<00:01, 22.38it/s]Computing mean-so-far PPL:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:03<00:01, 22.31it/s]Computing mean-so-far PPL:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:03<00:01, 22.29it/s]Computing mean-so-far PPL:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:03<00:00, 22.38it/s]Computing mean-so-far PPL:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:03<00:00, 22.46it/s]Computing mean-so-far PPL:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:03<00:00, 22.50it/s]Computing mean-so-far PPL:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 22.48it/s]Computing mean-so-far PPL:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:04<00:00, 22.47it/s]Computing mean-so-far PPL:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:04<00:00, 22.37it/s]Computing mean-so-far PPL:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:04<00:00, 22.42it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.07it/s]
wandb: updating run metadata; uploading artifact run-en8cmu4o-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_0b14911791207c759869.table.json
wandb: uploading artifact run-en8cmu4o-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÜ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÇ
wandb: grad_top/0_readout_proj.weight ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        grad_top/2_embed.weight ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñá‚ñÅ
wandb:               inference_mem_MB ‚ñÅ
wandb:                            +13 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 1.11266
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.26437
wandb: grad_top/0_readout_proj.weight 1.00084
wandb:  grad_top/1_readout_out.weight 0.48593
wandb:        grad_top/2_embed.weight 0.01417
wandb:               inference_mem_MB 4236.91776
wandb:                            +13 ...
wandb: 
wandb: üöÄ View run NGRC_LM(48.63M_d512_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251217-011605 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/en8cmu4o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_011639-en8cmu4o/logs
Inference time: 4.54s, Tokens/sec: 45141.13, Memory: 4236.92MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(48.63M_d512_lag20_poly3_rank512_lr0.0005_bs200_seq256_20251217-011605_report.txt
[2025-12-17 01:28:38,872] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 01:28:40,742] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_012910-2vr32z4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(52.56M_d512_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251217-012841
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/2vr32z4z
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=512, ngrc_lag=25, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=512, lag=25, poly_degree=3, phi_dim=38657, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 52.56M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.298852443695068, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 133012.2354498731, max_mem_mb: 26750.488064
[log] train loss: 5.447249889373779, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 136759.28120076156, max_mem_mb: 26750.488064
[log] train loss: 5.6848883628845215, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 136516.0822146932, max_mem_mb: 26750.489088
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   2%|‚ñè         | 2/100 [00:00<00:05, 17.28it/s]Computing mean-so-far PPL:   5%|‚ñå         | 5/100 [00:00<00:04, 20.45it/s]Computing mean-so-far PPL:   8%|‚ñä         | 8/100 [00:00<00:04, 21.01it/s]Computing mean-so-far PPL:  11%|‚ñà         | 11/100 [00:00<00:04, 21.47it/s]Computing mean-so-far PPL:  14%|‚ñà‚ñç        | 14/100 [00:00<00:03, 21.66it/s]Computing mean-so-far PPL:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 21.73it/s]Computing mean-so-far PPL:  20%|‚ñà‚ñà        | 20/100 [00:00<00:03, 22.00it/s]Computing mean-so-far PPL:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:01<00:03, 22.30it/s]Computing mean-so-far PPL:  26%|‚ñà‚ñà‚ñå       | 26/100 [00:01<00:03, 22.52it/s]Computing mean-so-far PPL:  29%|‚ñà‚ñà‚ñâ       | 29/100 [00:01<00:03, 22.59it/s]Computing mean-so-far PPL:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:01<00:02, 22.71it/s]Computing mean-so-far PPL:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:01<00:02, 22.67it/s]Computing mean-so-far PPL:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:01<00:02, 22.63it/s]Computing mean-so-far PPL:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:02, 22.68it/s]Computing mean-so-far PPL:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:01<00:02, 22.70it/s]Computing mean-so-far PPL:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:02<00:02, 22.72it/s]Computing mean-so-far PPL:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:02<00:02, 22.73it/s]Computing mean-so-far PPL:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:02<00:02, 22.84it/s]Computing mean-so-far PPL:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:02<00:01, 22.99it/s]Computing mean-so-far PPL:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:02<00:01, 23.03it/s]Computing mean-so-far PPL:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:02<00:01, 23.10it/s]Computing mean-so-far PPL:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:02<00:01, 23.02it/s]Computing mean-so-far PPL:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:03<00:01, 23.07it/s]Computing mean-so-far PPL:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:03<00:01, 22.96it/s]Computing mean-so-far PPL:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:03<00:01, 22.95it/s]Computing mean-so-far PPL:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:03<00:01, 22.97it/s]Computing mean-so-far PPL:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:03<00:00, 22.95it/s]Computing mean-so-far PPL:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:03<00:00, 22.95it/s]Computing mean-so-far PPL:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:03<00:00, 22.97it/s]Computing mean-so-far PPL:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:03<00:00, 22.99it/s]Computing mean-so-far PPL:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:04<00:00, 23.06it/s]Computing mean-so-far PPL:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:04<00:00, 22.97it/s]Computing mean-so-far PPL:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:04<00:00, 22.94it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.62it/s]
wandb: updating run metadata; uploading artifact run-2vr32z4z-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_654b45809dfa2e10fc72.table.json
wandb: uploading artifact run-2vr32z4z-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÅ
wandb:  grad_top/0_readout_out.weight ‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ
wandb: grad_top/0_readout_proj.weight ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 1.19427
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.24459
wandb:  grad_top/0_readout_out.weight 3.54797
wandb: grad_top/0_readout_proj.weight 1.06642
wandb:  grad_top/1_readout_out.weight 0.53743
wandb: grad_top/1_readout_proj.weight 3.51414
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(52.56M_d512_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251217-012841 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/2vr32z4z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_012910-2vr32z4z/logs
Inference time: 4.43s, Tokens/sec: 46254.01, Memory: 4302.65MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(52.56M_d512_lag25_poly3_rank512_lr0.0005_bs200_seq256_20251217-012841_report.txt
[2025-12-17 01:42:37,831] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-17 01:42:39,758] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251217_014309-b7ray2b1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(56.49M_d512_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251217-014240
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/b7ray2b1
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=200, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=256, total_tokens=100000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=512, ngrc_lag=30, ngrc_poly_degree=3, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name=None, api_file='api.txt', hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=512, lag=30, poly_degree=3, phi_dim=46337, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 56.49M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Repo card metadata block was not found. Setting CardData to empty.
[log] train loss: 6.391148090362549, step: 500, tokens_seen_global: 25600000, tokens_per_sec_global: 125808.7583439025, max_mem_mb: 28085.063168
[log] train loss: 5.504690647125244, step: 1000, tokens_seen_global: 51200000, tokens_per_sec_global: 128962.99503379573, max_mem_mb: 28085.063168
[log] train loss: 5.727652072906494, step: 1500, tokens_seen_global: 76800000, tokens_per_sec_global: 128441.43925716553, max_mem_mb: 28085.064192
HF upload skipped (token absent or repo not specified).
‚ñ∂Ô∏è babylm „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Åß mean-so-far-PPL „ÇíÊúÄÂ§ß 2048 „Éà„Éº„ÇØ„É≥„Åæ„ÅßË®àÊ∏¨„Åó„Åæ„Åô‚Ä¶
Computing mean-so-far PPL:   0%|          | 0/100 [00:00<?, ?it/s]Computing mean-so-far PPL:   2%|‚ñè         | 2/100 [00:00<00:05, 16.49it/s]Computing mean-so-far PPL:   5%|‚ñå         | 5/100 [00:00<00:04, 19.75it/s]Computing mean-so-far PPL:   8%|‚ñä         | 8/100 [00:00<00:04, 20.78it/s]Computing mean-so-far PPL:  11%|‚ñà         | 11/100 [00:00<00:04, 20.91it/s]Computing mean-so-far PPL:  14%|‚ñà‚ñç        | 14/100 [00:00<00:04, 21.06it/s]Computing mean-so-far PPL:  17%|‚ñà‚ñã        | 17/100 [00:00<00:03, 21.00it/s]Computing mean-so-far PPL:  20%|‚ñà‚ñà        | 20/100 [00:00<00:03, 21.06it/s]Computing mean-so-far PPL:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:01<00:03, 21.07it/s]Computing mean-so-far PPL:  26%|‚ñà‚ñà‚ñå       | 26/100 [00:01<00:03, 21.21it/s]Computing mean-so-far PPL:  29%|‚ñà‚ñà‚ñâ       | 29/100 [00:01<00:03, 21.22it/s]Computing mean-so-far PPL:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:01<00:03, 21.31it/s]Computing mean-so-far PPL:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [00:01<00:03, 21.38it/s]Computing mean-so-far PPL:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [00:01<00:02, 21.44it/s]Computing mean-so-far PPL:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:01<00:02, 21.45it/s]Computing mean-so-far PPL:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:02<00:02, 21.42it/s]Computing mean-so-far PPL:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:02<00:02, 21.32it/s]Computing mean-so-far PPL:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [00:02<00:02, 21.39it/s]Computing mean-so-far PPL:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:02<00:02, 21.47it/s]Computing mean-so-far PPL:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:02<00:02, 21.48it/s]Computing mean-so-far PPL:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [00:02<00:01, 21.53it/s]Computing mean-so-far PPL:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [00:02<00:01, 21.57it/s]Computing mean-so-far PPL:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:03<00:01, 21.66it/s]Computing mean-so-far PPL:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:03<00:01, 21.71it/s]Computing mean-so-far PPL:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [00:03<00:01, 21.76it/s]Computing mean-so-far PPL:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [00:03<00:01, 21.74it/s]Computing mean-so-far PPL:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:03<00:01, 21.71it/s]Computing mean-so-far PPL:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:03<00:00, 21.79it/s]Computing mean-so-far PPL:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:03<00:00, 21.82it/s]Computing mean-so-far PPL:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [00:04<00:00, 21.84it/s]Computing mean-so-far PPL:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:04<00:00, 21.86it/s]Computing mean-so-far PPL:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [00:04<00:00, 21.80it/s]Computing mean-so-far PPL:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [00:04<00:00, 21.77it/s]Computing mean-so-far PPL:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [00:04<00:00, 21.85it/s]Computing mean-so-far PPL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 21.45it/s]
wandb: updating run metadata; uploading artifact run-b7ray2b1-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1955_5bab964a2296cd21bbc7.table.json
wandb: uploading artifact run-b7ray2b1-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:                     current_lr ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       grad/preclip/global_norm ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/inf_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            grad/preclip/layers ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/nan_count ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         grad/preclip/zero_frac ‚ñá‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñá‚ñÜ
wandb:  grad_top/0_readout_out.weight ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: grad_top/0_readout_proj.weight ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  grad_top/1_readout_out.weight ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñá‚ñÅ
wandb: grad_top/1_readout_proj.weight ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                            +15 ...
wandb: 
wandb: Run summary:
wandb:                     current_lr 0
wandb:       grad/preclip/global_norm 1.24144
wandb:         grad/preclip/inf_count 0
wandb:            grad/preclip/layers 3
wandb:         grad/preclip/nan_count 0
wandb:         grad/preclip/zero_frac 0.22756
wandb:  grad_top/0_readout_out.weight 2.6376
wandb: grad_top/0_readout_proj.weight 1.10679
wandb:  grad_top/1_readout_out.weight 0.56214
wandb: grad_top/1_readout_proj.weight 2.32406
wandb:                            +15 ...
wandb: 
wandb: üöÄ View run NGRC_LM(56.49M_d512_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251217-014240 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/b7ray2b1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251217_014309-b7ray2b1/logs
Inference time: 4.67s, Tokens/sec: 43867.86, Memory: 4368.71MB
üìÑ Training report written to ./reports_ngrc/NGRC_LM(56.49M_d512_lag30_poly3_rank512_lr0.0005_bs200_seq256_20251217-014240_report.txt
