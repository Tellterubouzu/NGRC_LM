/work/gp36/b20072/.bashrc: line 7: /work/gp36/b20072/.cargo/env: No such file or directory
/work/gp36/b20072/.bashrc: line 10: rbenv: command not found
/work/gp36/b20072/.bash_profile: line 9: /work/gp36/b20072/.cargo/env: No such file or directory
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 4r9ewop2
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251222_102518-4r9ewop2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run NGRC_LM(113.51M_10BT_lr5e-4_d2048_poly1_bs198)3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: üöÄ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/4r9ewop2
Namespace(local_rank=0, deepspeed_config='ds_config.json', use_deepspeed=False, local_batch_size=192, use_gpu_amount=1, learning_rate=0.0005, validate_every_steps=200, save_checkpoint_every_steps=200, generate_every=1000, seq_len=512, total_tokens=10000000000.0, epochs=1, grad_clip_norm=0.0, beta1=0.9, beta2=0.95, weight_decay=0.1, tokenizer_path='meta-llama/Llama-2-7b-hf', ngrc_d_model=2048, ngrc_lag=10, ngrc_poly_degree=1, ngrc_max_cross_terms=256, ngrc_readout_rank=512, ngrc_embed_frozen=False, ngrc_training='sgd', ngrc_loss='ce', ngrc_ridge_alpha=0.001, ngrc_ridge_max_batches=200, dataset_path='HuggingFaceFW/fineweb-edu', val_dataset_path='vesteinn/babylm', wandb_project='NGRC_LanguageModel', wandb_run_name='NGRC_LM(113.51M_10BT_lr5e-4_d2048_poly1_bs198)3', api_file='api.txt', log_grad=False, hf_repo=None, hf_private=True, enable_compile=False)
NGRC_LM initialized: vocab=32000, d_model=2048, lag=10, poly_degree=1, phi_dim=20737, readout_rank=512, embed_trainable=True, loss_type=ce
parameter count: 92.54M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
