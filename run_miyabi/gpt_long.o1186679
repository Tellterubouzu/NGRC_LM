[2025-12-18 17:49:08,853] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-18 17:49:10,962] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Repo card metadata block was not found. Setting CardData to empty.
wandb: Currently logged in as: shimomura-teruki174 (telutelu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.4
wandb: Run data is saved locally in /work/gp36/b20072/NGRC_LM/src/wandb/run-20251218_174945-j97s3wnx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run GPT(RoPE)_116.72M_batch_size110_seq_len512_20251218-174945
wandb: â­ï¸ View project at https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: ğŸš€ View run at https://wandb.ai/telutelu/NGRC_LanguageModel/runs/j97s3wnx
parameter count: 116.72M
[data] HF streaming dataset HuggingFaceFW/fineweb-edu
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: WARNING Tried to log to step 1001 that is less than the current step 1003. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1002 that is less than the current step 1003. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Repo card metadata block was not found. Setting CardData to empty.
wandb: updating run metadata; uploading history steps 1775-1775, summary, console lines 5-6; uploading artifact run-j97s3wnx-mean_so_far_ppl_curve_table; uploading media/table/mean_so_far_ppl_curve_table_1777_30c44c7e5cd4f747c6e4.table.json
wandb: uploading artifact run-j97s3wnx-mean_so_far_ppl_curve_table
wandb: 
wandb: Run history:
wandb:            current_lr â–ƒâ–„â–„â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:      inference_mem_MB â–
wandb:        inference_time â–
wandb: inference_tok_per_sec â–
wandb:            max_mem_mb â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             max_steps â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          seenedtokens â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   seenedtokens_global â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                  step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:        tokens_per_sec â–â–„â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    +5 ...
wandb: 
wandb: Run summary:
wandb:            current_lr 0
wandb:      inference_mem_MB 5642.98189
wandb:        inference_time 13.10426
wandb: inference_tok_per_sec 31257.01002
wandb:            max_mem_mb 68828.91366
wandb:             max_steps 1776
wandb:          seenedtokens 100024320
wandb:   seenedtokens_global 100024320
wandb:                  step 1776
wandb:        tokens_per_sec 178299.26624
wandb:                    +5 ...
wandb: 
wandb: ğŸš€ View run GPT(RoPE)_116.72M_batch_size110_seq_len512_20251218-174945 at: https://wandb.ai/telutelu/NGRC_LanguageModel/runs/j97s3wnx
wandb: â­ï¸ View project at: https://wandb.ai/telutelu/NGRC_LanguageModel
wandb: Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251218_174945-j97s3wnx/logs
HF upload skipped (token absent or repo not specified).
â–¶ï¸ babylm ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ mean-so-far-PPL ã‚’æœ€å¤§ 4096 ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§è¨ˆæ¸¬ã—ã¾ã™â€¦
â†’ 4000 ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆ
Inference time: 13.10s, Tokens/sec: 31257.01, Memory: 5642.98MB
ğŸ“„ Training report written to ./GPT(RoPE)_116.72M_batch_size110_seq_len512_20251218-174945_report.txt
